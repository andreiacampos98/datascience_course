{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19c1b88",
   "metadata": {},
   "source": [
    "# Building a predictive model - Part I\n",
    "\n",
    "[1. Import data (Data Integration)](#1st-bullet)<br>\n",
    "[2. Explore data (Data Access, Exploration and Understanding)](#2nd-bullet)<br>\n",
    "[3. Modify data (Data Preparation)](#3rd-bullet)<br>\n",
    "- [3.1. Feature Selection](#4th-bullet)<br>\n",
    "    - [3.1.1. Categorical Variables](#5th-bullet)<br>\n",
    "        - [3.1.1.1. Chi-Square](#6th-bullet)<br>\n",
    "    - [3.1.2 Numerical Variables](#7th-bullet)<br>\n",
    "        - [3.1.2.1. Variance](#8th-bullet)<br>\n",
    "        - [3.1.2.2. Spearman Correlation](#9th-bullet)<br>\n",
    "        - [3.1.2.3. Feature Importance in Decision Trees](#10th-bullet)<br>\n",
    "        - [3.1.2.4. Recursive Feature Elimination with Logistic Regression](#11th-bullet)<br>\n",
    "        - [3.1.2.5. Lasso Regression](#12th-bullet)<br>\n",
    "\n",
    "\n",
    "<img src=\"Images/process_ML.png\" style=\"height:70px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aecf66",
   "metadata": {},
   "source": [
    "__`Step 1`__ Import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec8f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "############################################ FEATURE SELECTION ############################################\n",
    "#!pip install scipy\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency # filter method\n",
    "from sklearn.feature_selection import RFE # wrapper method\n",
    "from sklearn.linear_model import LogisticRegression # (this is one possible model to apply inside RFE)\n",
    "from sklearn.linear_model import LogisticRegressionCV # embedded method - LassoCV would be used in a regression problem \n",
    "from sklearn.tree import DecisionTreeClassifier # embedded method\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c965128",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1st-bullet\">\n",
    "\n",
    "# 1. Import data (Data Integration)\n",
    "\n",
    "</a>\n",
    "\n",
    "<img src=\"Images/step1.png\" style=\"height:70px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f95ab",
   "metadata": {},
   "source": [
    "__`Step 2`__ Import the train dataset and define the index as the `Custid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c667c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_Birth</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Income</th>\n",
       "      <th>Rcn</th>\n",
       "      <th>Frq</th>\n",
       "      <th>Mnt</th>\n",
       "      <th>Clothes</th>\n",
       "      <th>Kitchen</th>\n",
       "      <th>SmallAppliances</th>\n",
       "      <th>HouseKeeping</th>\n",
       "      <th>Toys</th>\n",
       "      <th>NetPurchase</th>\n",
       "      <th>CatPurchase</th>\n",
       "      <th>Recomendation</th>\n",
       "      <th>CostPerContact</th>\n",
       "      <th>RevenuePerPositiveAnswer</th>\n",
       "      <th>DepVar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>1984</td>\n",
       "      <td>M</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>39300.45</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>43.68</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5261</th>\n",
       "      <td>1941</td>\n",
       "      <td>F</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>109484.55</td>\n",
       "      <td>84</td>\n",
       "      <td>34</td>\n",
       "      <td>1670.24</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9733</th>\n",
       "      <td>1966</td>\n",
       "      <td>F</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>BigConfusion</td>\n",
       "      <td>1</td>\n",
       "      <td>48260.10</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>48.88</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5922</th>\n",
       "      <td>1947</td>\n",
       "      <td>F</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>116437.65</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>1659.84</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>1993</td>\n",
       "      <td>F</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>19188.75</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>38.48</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>1948</td>\n",
       "      <td>M</td>\n",
       "      <td>2n Cycle</td>\n",
       "      <td>Together</td>\n",
       "      <td>0</td>\n",
       "      <td>102802.35</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>1256.32</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6452</th>\n",
       "      <td>1971</td>\n",
       "      <td>M</td>\n",
       "      <td>Graduation</td>\n",
       "      <td>Single</td>\n",
       "      <td>1</td>\n",
       "      <td>59131.80</td>\n",
       "      <td>431</td>\n",
       "      <td>7</td>\n",
       "      <td>19.76</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>1946</td>\n",
       "      <td>F</td>\n",
       "      <td>2n Cycle</td>\n",
       "      <td>Married</td>\n",
       "      <td>0</td>\n",
       "      <td>103750.50</td>\n",
       "      <td>64</td>\n",
       "      <td>41</td>\n",
       "      <td>2058.16</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8868</th>\n",
       "      <td>1943</td>\n",
       "      <td>F</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Widow</td>\n",
       "      <td>0</td>\n",
       "      <td>109616.85</td>\n",
       "      <td>44</td>\n",
       "      <td>36</td>\n",
       "      <td>1794.00</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>1982</td>\n",
       "      <td>M</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Together</td>\n",
       "      <td>1</td>\n",
       "      <td>55740.30</td>\n",
       "      <td>191</td>\n",
       "      <td>9</td>\n",
       "      <td>20.80</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Year_Birth Gender   Education Marital_Status  Dependents     Income  \\\n",
       "Custid                                                                        \n",
       "1903          1984      M  Graduation         Single           1   39300.45   \n",
       "5261          1941      F  Graduation        Married           0  109484.55   \n",
       "9733          1966      F  Graduation   BigConfusion           1   48260.10   \n",
       "5922          1947      F       Basic        Married           0  116437.65   \n",
       "2432          1993      F  Graduation         Single           1   19188.75   \n",
       "...            ...    ...         ...            ...         ...        ...   \n",
       "4669          1948      M    2n Cycle       Together           0  102802.35   \n",
       "6452          1971      M  Graduation         Single           1   59131.80   \n",
       "1944          1946      F    2n Cycle        Married           0  103750.50   \n",
       "8868          1943      F         PhD          Widow           0  109616.85   \n",
       "9186          1982      M         PhD       Together           1   55740.30   \n",
       "\n",
       "        Rcn  Frq      Mnt  Clothes  Kitchen  SmallAppliances  HouseKeeping  \\\n",
       "Custid                                                                       \n",
       "1903     10    9    43.68       73        0               24             3   \n",
       "5261     84   34  1670.24       78        0               19             2   \n",
       "9733     92   10    48.88       86        1               10             2   \n",
       "5922     55   33  1659.84       18       36               16            13   \n",
       "2432     52   11    38.48       15       39               23            22   \n",
       "...     ...  ...      ...      ...      ...              ...           ...   \n",
       "4669     42   28  1256.32       30        7               40             2   \n",
       "6452    431    7    19.76       83        1               15             1   \n",
       "1944     64   41  2058.16       19       23               33            23   \n",
       "8868     44   36  1794.00       42       14               39             2   \n",
       "9186    191    9    20.80       49        4               39             2   \n",
       "\n",
       "        Toys  NetPurchase  CatPurchase  Recomendation  CostPerContact  \\\n",
       "Custid                                                                  \n",
       "1903       0           53           47              3               2   \n",
       "5261       1           16           84              5               2   \n",
       "9733       2           67           33              3               2   \n",
       "5922      17            7           93              5               2   \n",
       "2432       1           62           38              5               2   \n",
       "...      ...          ...          ...            ...             ...   \n",
       "4669      21           18           82              5               2   \n",
       "6452       0           53           47              3               2   \n",
       "1944       2           14           86              6               2   \n",
       "8868       3           13           87              4               2   \n",
       "9186       6           64           36              2               2   \n",
       "\n",
       "        RevenuePerPositiveAnswer  DepVar  \n",
       "Custid                                    \n",
       "1903                          15       0  \n",
       "5261                          15       1  \n",
       "9733                          15       0  \n",
       "5922                          15       0  \n",
       "2432                          15       0  \n",
       "...                          ...     ...  \n",
       "4669                          15       0  \n",
       "6452                          15       0  \n",
       "1944                          15       0  \n",
       "8868                          15       0  \n",
       "9186                          15       0  \n",
       "\n",
       "[2000 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'Data/data.xlsx')\n",
    "data.set_index('Custid', inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc5a0d",
   "metadata": {},
   "source": [
    "A DepVar é a variavel dependente que vamos tentar prever. Esta é a adesão a uma campanha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78713929",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2nd-bullet\">\n",
    "\n",
    "# 2. Explore data (Data access, exploration and understanding)\n",
    "\n",
    "</a>\n",
    "<img src=\"Images/step2.png\" style=\"height:70px\">\n",
    "\n",
    "Remember, this step is very important as it is at this stage that you will really look into the data that you have. Generally speaking, if you do well at this stage, the following stages will be more smooth.\n",
    "\n",
    "Moreover, you should also take the time to find meaningful patterns on the data: what interesting relationships can be found between the variables and how can that knowledge be inform your future decisions. \n",
    "\n",
    "__`Step 3`__ Check if you have missing values in your data with the method `info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052a4a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2000 entries, 1903 to 9186\n",
      "Data columns (total 20 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Year_Birth                2000 non-null   int64  \n",
      " 1   Gender                    2000 non-null   object \n",
      " 2   Education                 1993 non-null   object \n",
      " 3   Marital_Status            1989 non-null   object \n",
      " 4   Dependents                2000 non-null   int64  \n",
      " 5   Income                    1944 non-null   float64\n",
      " 6   Rcn                       2000 non-null   int64  \n",
      " 7   Frq                       2000 non-null   int64  \n",
      " 8   Mnt                       2000 non-null   float64\n",
      " 9   Clothes                   2000 non-null   int64  \n",
      " 10  Kitchen                   2000 non-null   int64  \n",
      " 11  SmallAppliances           2000 non-null   int64  \n",
      " 12  HouseKeeping              2000 non-null   int64  \n",
      " 13  Toys                      2000 non-null   int64  \n",
      " 14  NetPurchase               2000 non-null   int64  \n",
      " 15  CatPurchase               2000 non-null   int64  \n",
      " 16  Recomendation             2000 non-null   int64  \n",
      " 17  CostPerContact            2000 non-null   int64  \n",
      " 18  RevenuePerPositiveAnswer  2000 non-null   int64  \n",
      " 19  DepVar                    2000 non-null   int64  \n",
      "dtypes: float64(2), int64(15), object(3)\n",
      "memory usage: 328.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da233d",
   "metadata": {},
   "source": [
    "__Conclusion:__ We can verify that we have missing values on Education, Marital_Status and Income. We need to address this in further steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57511c06",
   "metadata": {},
   "source": [
    "__`Step 4`__ Check if you have misclassifications in your categorical data using `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25395ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marital_Status\n",
       "Married         725\n",
       "Together        487\n",
       "Single          479\n",
       "Divorced        193\n",
       "Widow            87\n",
       "BigConfusion     18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Marital_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af310e2",
   "metadata": {},
   "source": [
    "__Possible Solution:__ We are going to replace BigConfusion with the mode and assign all `Together` people as `Married`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09530a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender\n",
       "F    1519\n",
       "M     478\n",
       "?       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c11de8",
   "metadata": {},
   "source": [
    "__Possible Solution:__ We are going to replace `?` with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7373790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Education\n",
       "Graduation    987\n",
       "2n Cycle      339\n",
       "Master        290\n",
       "Basic         236\n",
       "PhD           135\n",
       "OldSchool       6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fae255",
   "metadata": {},
   "source": [
    "__Possible Solution:__ We are going to replace `OldSchool` with the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf26e13",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3rd-bullet\">\n",
    "\n",
    "# 3. Modify data (Data preparation)\n",
    "\n",
    "</a>\n",
    "<img src=\"Images/step3.png\" style=\"height:70px\">\n",
    "\n",
    "Remember that your decisions at this step should be exclusively informed by your **training data**. While you will need to split your data between training and validation, how that split will be made and how to apply the approppriate transformations will depend on the type of model assessment solution you select for your project (each has its own set of advantages and disadvantages that you need to consider). **Please find a list of possible methods for model assessment below**: \n",
    "\n",
    "1. **Holdout method**\n",
    "2. **Repeated Holdout method**\n",
    "3. **K-Fold Cross-Validation**\n",
    "4. **One Leave Out**\n",
    "\n",
    "In this dataset, we are going to use K-Fold Cross-Validation, since it is the one more appropriate for the size of the dataset that we are using.\n",
    "\n",
    "<img src=\"Images/KFold.png\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5f2e7",
   "metadata": {},
   "source": [
    "__`Step 5`__ Import `StratifiedKFold` from `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd03cd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b0318",
   "metadata": {},
   "source": [
    "__`Step 6`__ Create a new instance of StratifiedKFold named as `skf`, with the following hyperparameters:\n",
    "- n_splits = 5\n",
    "- random_state = 99\n",
    "- shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "987ad7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, random_state = 99, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e6f33",
   "metadata": {},
   "source": [
    "__`Step 7`__ To apply any of the model assessment techniques previously mentioned, we need to define at first what are our independent variables (`X`) and the target (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6726abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('DepVar', axis = 1)\n",
    "y = data['DepVar'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ecb1a",
   "metadata": {},
   "source": [
    "x - faz fit\n",
    "y - para predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8620df2-e9b2-4b84-8e0b-d89061fe364d",
   "metadata": {},
   "source": [
    "__`Step 8`__ Split `X` and `y` using `train_test_split` method to leave an independent test set out of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bac26c9d-4315-41d6-994a-70f32611275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, \n",
    "                                        train_size = 0.9, \n",
    "                                        random_state = 99, \n",
    "                                        stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc03cd",
   "metadata": {},
   "source": [
    "For the remaining steps, we are going to deal with our data as if we had two different data sets: the train and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880c163",
   "metadata": {},
   "source": [
    "IMPORTANT\n",
    "Só vamos tratar os valores missing depois de dividir os dados para treino e teste.\n",
    "De modo a garantir que não há data lakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba354dd7",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4th-bullet\">\n",
    "\n",
    "## 3.1. Feature Selection\n",
    "    \n",
    "</a>\n",
    "\n",
    "In this step we are going to apply Feature Selection, namely:\n",
    "- __Variance (Filter Method)__ \n",
    "- __Spearman Correlation (Filter Method)__ \n",
    "- __Recursive Feature Elimination (RFE) (Wrapper Method)__\n",
    "- __Lasso Regression (Embedded Method)__\n",
    "- __Decision Trees (Embedded Method)__\n",
    "\n",
    "Since we know that regressions are really sensible to correlated features, we should remove the correlated features before applying those techniques, namely Lasso Regression and Recursive Feature Elimination if the estimator used is a regression model.\n",
    "\n",
    "In that way, we are going to apply the feature selection techniques in the following order: <br>\n",
    "__(1) Variance__ - To  understand if there are any constant variables;<br>\n",
    "__(2) Spearman Correlation__ - To verify if there are any correlated features;<br>\n",
    "__(3) Decision Trees__ - To just keep one variable from a group of correlated features;<br>\n",
    "__(4) RFE__ - To iteratively select features by considering subsets of features;<br>\n",
    "__(5) Lasso__ - To identify and select important features in a dataset;<br>\n",
    "\n",
    "Furthermore, to apply Feature selection, we need at first to identify the data type of each variable, since different techniques are specific to different data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30c217",
   "metadata": {},
   "source": [
    "| Predictor | Data Type | \n",
    "| --- | --- | \n",
    "| Gender | Categorical |\n",
    "| Education | Categorical |\n",
    "| Marital_Status | Categorical |\n",
    "| Dependents | Binary |\n",
    "| Income | Continuous |\n",
    "| Rcn | Continuous |\n",
    "| Frq | Continuous |\n",
    "| Mnt | Continuous |\n",
    "| Clothes | Continuous |\n",
    "| Kitchen | Continuous |\n",
    "| SmallAppliances | Continuous |\n",
    "| HouseKeeping | Continuous |\n",
    "| Toys | Continuous |\n",
    "| NetPurchase | Continuous |\n",
    "| CatPurchase | Continuous |\n",
    "| Recomendation | Ordinal |\n",
    "| CostPerContact | Continuous |\n",
    "| RevenuePerPositiveAnswer | Continuous |\n",
    "| --- | --- |\n",
    "| DepVar | Binary |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069b0f5",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5th-bullet\">\n",
    "\n",
    "### 3.1.1. Categorical features\n",
    "    \n",
    "</a>\n",
    "\n",
    "We are going to use chi-square to understand which categorical variables should we keep. <br>\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"6th-bullet\">\n",
    "\n",
    "#### __`3.1.1.1. Chi-Square (Filter Method) - For categorical data`__\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c6385",
   "metadata": {},
   "source": [
    "__`Step 8`__ - Create a function named as `apply_chisquare` to select features from the categorical variables using `chi2_contigency`from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "052c457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chisquare(X,y,var,alpha=0.05):        \n",
    "    dfObserved = pd.crosstab(y,X) \n",
    "    chi2, p, dof, expected = stats.chi2_contingency(dfObserved.values)\n",
    "    dfExpected = pd.DataFrame(expected, columns=dfObserved.columns, index = dfObserved.index)\n",
    "    if p<alpha:\n",
    "        result=\"{0} is IMPORTANT for Prediction\".format(var)\n",
    "    else:\n",
    "        result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(var)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac32e2",
   "metadata": {},
   "source": [
    "Now it is time to select the categorical features to keep. <br> \n",
    "Remember that we want to select features that are important, independently of the subset of data that is being used for train. <br> \n",
    "For that, we need to apply our function previously defined inside the StratifiedKFold.\n",
    "\n",
    "__`Step 9`__ - Create a function named as `select_best_cat_features` that receives the independent variables (X) and the target (y). <br>\n",
    "Inside of this function, follow these steps:<br>\n",
    "- Apply the previously defined StratifiedKFold as \"skf\" for cross-validation.\n",
    "- Fill missing data, focusing on the categorical features.\n",
    "- Correct misclassifications in the categorical data that were identified during exploration.\n",
    "- Call the \"apply_chisquare\" function on your categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "928ae883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440\n",
      "1440\n",
      "1440\n",
      "1440\n",
      "1440\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(X, y):\n",
    "    print(len(train_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7121322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "360\n",
      "360\n",
      "360\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in skf.split(X, y):\n",
    "    print(len(val_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22be6b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_cat_features(X,y):\n",
    "    count = 1\n",
    "    \n",
    "    ############################################## APPLY SKF ######################################################\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        ####################################### FILL MISSING DATA #################################################\n",
    "        # Fill missing values (mode in categorical data)\n",
    "        X_train['Marital_Status'].fillna(X_train['Marital_Status'].mode()[0], inplace = True)\n",
    "        X_train['Education'].fillna(X_train['Education'].mode()[0], inplace = True)\n",
    "        \n",
    "        #################################### CORRECT MISCLASSIFICAIONS ############################################\n",
    "        # Corre bct misclassifications in the Marital Status in training data\n",
    "        X_train['Marital_Status'] = X_train['Marital_Status'].replace('BigConfusion',X_train['Marital_Status'].mode()[0])\n",
    "        X_train['Marital_Status'] = X_train['Marital_Status'].replace('Together','Married')\n",
    "        # Correct misclassifications in the Gender in training data\n",
    "        X_train['Gender'] = X_train['Gender'].replace('?',X_train['Gender'].mode()[0])\n",
    "        # Correct misclassifications in the Education in training data\n",
    "        X_train['Education'] = X_train['Education'].replace('OldSchool',X_train['Education'].mode()[0])\n",
    "        \n",
    "        #################################### SELECT FEATURES WITH CHI-SQUARE #######################################        \n",
    "        print('_________________________________________________________________________________________________\\n')\n",
    "        print('                                     SPLIT ' + str(count) + '                                    ')\n",
    "        print('_________________________________________________________________________________________________')\n",
    "\n",
    "        # check which features to use using chi-square\n",
    "        X_train_cat = X_train[['Gender','Education','Marital_Status']].copy()\n",
    "        for var in X_train_cat:\n",
    "            apply_chisquare(X_train_cat[var],y_train, var)\n",
    "            \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4bd4a",
   "metadata": {},
   "source": [
    "__`Step 9B`__ - Call your function `select_best_cat_features` passing as arguments your independent variables and your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0f438d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________________\n",
      "\n",
      "                                     SPLIT 1                                    \n",
      "_________________________________________________________________________________________________\n",
      "Gender is NOT an important predictor. (Discard Gender from model)\n",
      "Education is NOT an important predictor. (Discard Education from model)\n",
      "Marital_Status is IMPORTANT for Prediction\n",
      "_________________________________________________________________________________________________\n",
      "\n",
      "                                     SPLIT 2                                    \n",
      "_________________________________________________________________________________________________\n",
      "Gender is NOT an important predictor. (Discard Gender from model)\n",
      "Education is NOT an important predictor. (Discard Education from model)\n",
      "Marital_Status is IMPORTANT for Prediction\n",
      "_________________________________________________________________________________________________\n",
      "\n",
      "                                     SPLIT 3                                    \n",
      "_________________________________________________________________________________________________\n",
      "Gender is NOT an important predictor. (Discard Gender from model)\n",
      "Education is NOT an important predictor. (Discard Education from model)\n",
      "Marital_Status is IMPORTANT for Prediction\n",
      "_________________________________________________________________________________________________\n",
      "\n",
      "                                     SPLIT 4                                    \n",
      "_________________________________________________________________________________________________\n",
      "Gender is NOT an important predictor. (Discard Gender from model)\n",
      "Education is NOT an important predictor. (Discard Education from model)\n",
      "Marital_Status is IMPORTANT for Prediction\n",
      "_________________________________________________________________________________________________\n",
      "\n",
      "                                     SPLIT 5                                    \n",
      "_________________________________________________________________________________________________\n",
      "Gender is NOT an important predictor. (Discard Gender from model)\n",
      "Education is NOT an important predictor. (Discard Education from model)\n",
      "Marital_Status is IMPORTANT for Prediction\n"
     ]
    }
   ],
   "source": [
    "select_best_cat_features(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a28af",
   "metadata": {},
   "source": [
    "#### What can we conclude?\n",
    "\n",
    "| Predictor | Chi-Square | What to do? (One possible way to \"solve\")|\n",
    "| --- | --- | --- |\n",
    "| Gender | 5 NO | Remove |\n",
    "| Education | 5 NO | Remove|\n",
    "| Marital_Status | 5 YES|Keep|\n",
    "\n",
    "We should keep only the Marital_Status.\n",
    "\n",
    "__`Step 10`__ - Remove the variables that are not considered important from chi-square, namely `gender` and `education`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Gender','Education'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3847ca2",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7th-bullet\">\n",
    "\n",
    "### 3.1.2. Numerical features\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf816957",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8th-bullet\">\n",
    "\n",
    "__`3.1.2.1. Variance (Filter Method) - For continuous, ordinal and binary data`__\n",
    "    \n",
    "</a>\n",
    "\n",
    "We can use variance to understand if our numerical data and ordinal data is constant or quasi-constant.\n",
    "\n",
    "__`Step 11`__ - Create a function named as `apply_variance` that receives your training data, and outputs the variance for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_variance(X_train):\n",
    "    print(X_train.var(numeric_only=True).apply(lambda x: f\"{x:.5f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae9383",
   "metadata": {},
   "source": [
    "Similarly to what we have done in the chi-square test, we will now examine the variance of the independent features in the training data, regardless of the specific train subset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ed23f",
   "metadata": {},
   "source": [
    "__`Step 12`__ - Create a function named as `select_features_variance` that receives the independent variables (X) and the target (y). <br>\n",
    "Inside of this function, follow these steps:<br>\n",
    "- Apply the previously defined StratifiedKFold as \"skf\" for cross-validation.\n",
    "- Fill missing data, focusing this time on the numerical variables.\n",
    "- Call the \"apply_variance\" function on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_variance(X,y):\n",
    "    count = 1\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        ####################################### FILL MISSING DATA #################################################\n",
    "        # Fill missing values (median in numerical data)\n",
    "        X_train['Income'].fillna(X_train['Income'].median(), inplace = True)\n",
    "        \n",
    "        ######################################### SELECT FEATURES #################################################        \n",
    "        print('_________________________________________________________________________________________________\\n')\n",
    "        print('                                     SPLIT ' + str(count) + '                                    ')\n",
    "        print('_________________________________________________________________________________________________')\n",
    "\n",
    "        # check which features to use using variance\n",
    "        apply_variance(X_train)\n",
    "            \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83603a",
   "metadata": {},
   "source": [
    "__`Step 12B`__ - Call your function `select_features_variance` passing as arguments your independent variables and your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features_variance(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664e8ec3",
   "metadata": {},
   "source": [
    "#### What can we conclude?\n",
    "\n",
    "- `CostPerContact` and `RevenuePerPositiveAnswer` are constant features. We should remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a627435",
   "metadata": {},
   "source": [
    "__`Step 13`__ - Remove the variables that are constant, namely `CostPerContact` and `RevenuePerPositiveAnswer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc3125",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['CostPerContact', 'RevenuePerPositiveAnswer'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e4c23",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9th-bullet\">\n",
    "\n",
    "__`3.1.2.2. Spearman Correlation (Filter Method) - For continuous and ordinal data`__\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5144a",
   "metadata": {},
   "source": [
    "We can use Spearman correlation to determine if we have redundant variables that should be removed due to their high correlation. \n",
    "\n",
    "However, it's important to note that applying Spearman correlation to binary variables may not yield meaningful or accurate results. This correlation measure is more suitable for continuous and ordinal data. \n",
    "\n",
    "Therefore:\n",
    "- Since our target is binary, we should not consider the correlation of the independent variables with the target variable to understand if they are \"important\". \n",
    "- In addition, the variable \"Dependents\" should also be excluded from the correlation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa537813",
   "metadata": {},
   "source": [
    "__`Step 14`__ - Create a function named as `cor_heatmap` that receives a correlation matrix and: <br>\n",
    "- create a figure with figsize=(9,6)  <br>\n",
    "- plot an heatmap with the correlation matrix received, where annot = True, cmap = plt.cm.Reds and fmt='.1' <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_heatmap(cor):\n",
    "    plt.figure(figsize=(9,6))\n",
    "    sns.heatmap(data = cor, annot = True, cmap = plt.cm.Reds, fmt='.1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551a185",
   "metadata": {},
   "source": [
    "__`Step 15`__ - Create a function named `apply_correlation` that takes your training data as input. In this function:\n",
    "\n",
    "- Remove the variable `Dependents` as it is binary and not suitable for the correlation matrix.\n",
    "- Create an object named `matrix` to store the results of the Spearman correlation matrix for our data.\n",
    "- Call the previously defined function `cor_heatmap` to generate the correlation heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correlation(X_train):\n",
    "    correlation_data = X_train.drop(['Dependents'], axis = 1).copy()\n",
    "    matrix = correlation_data.corr(method = 'spearman', numeric_only=True)\n",
    "    cor_heatmap(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caefdd6",
   "metadata": {},
   "source": [
    "nao se deve usar variaveis binárias nas correlações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a6df8",
   "metadata": {},
   "source": [
    "Similarly to what we have done in the chi-square test and in the variance, we will now examine the correlation of the independent features in the training data, regardless of the specific train subset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248d6012",
   "metadata": {},
   "source": [
    "__`Step 16`__ - Create a function named as `redundant_features` that receives the independent variables (X) and the target (y). <br>\n",
    "Inside of this function, follow these steps:<br>\n",
    "- Apply the previously defined StratifiedKFold as \"skf\" for cross-validation.\n",
    "- Fill missing data, focusing this time on the numerical variables.\n",
    "- Call the \"apply_correlation\" function on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124fe7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redundant_features(X,y):\n",
    "    count = 1\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        ####################################### FILL MISSING DATA #################################################\n",
    "        # Fill missing values in training data\n",
    "        X_train['Income'].fillna(X_train['Income'].median(), inplace = True)\n",
    "        \n",
    "        ######################################### SELECT FEATURES #################################################        \n",
    "        print('_________________________________________________________________________________________________\\n')\n",
    "        print('                                     SPLIT ' + str(count) + '                                    ')\n",
    "        print('_________________________________________________________________________________________________')\n",
    "        \n",
    "        # check which features to use using spearman correlation\n",
    "        apply_correlation(X_train)\n",
    "        \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a9dfc",
   "metadata": {},
   "source": [
    "__`Step 16B`__ - Call your function `redundant_features` passing as arguments your independent variables and your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3964345",
   "metadata": {},
   "outputs": [],
   "source": [
    "redundant_features(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9f6a1",
   "metadata": {},
   "source": [
    "### Redundant Variables - Spearman Correlation (Correlation higher than |0.8|)\n",
    "\n",
    "| Combination | Split 1 | Split 2 | Split 3 | Split 4 | Split 5 |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Year_Birth Vs Income | -0.9 | -0.9 | -0.9 | -0.9 | -0.9 |\n",
    "| Year_Birth Vs Frq | -0.8 | -0.8 | -0.8 | -0.8 | -0.8 |\n",
    "| Year_Birth Vs Mnt | -0.8 | -0.8 | -0.8 | -0.8 | -0.8 |\n",
    "| Year_Birth Vs NetPurchase | 0.8 | 0.8 | 0.7 | 0.8 | 0.8 |\n",
    "| Year_Birth Vs CatPurchase | -0.8 | -0.8 | -0.7 | -0.8 | -0.8 |\n",
    "| Income Vs Frq | 0.8 | 0.8 | 0.8 | 0.8 | 0.8 |\n",
    "| Income Vs Mnt | 0.9 | 0.9 | 0.9 | 0.9 | 0.9 |\n",
    "| __Frq Vs Mnt__ | __1.0__ | __1.0__ | __1.0__ | __1.0__ | __1.0__ |\n",
    "| __NetPurchase Vs CatPurchase__ | __1.0__ | __1.0__ | __1.0__ | __1.0__ | __1.0__ |\n",
    "\n",
    "\n",
    "In this case, we can see that there are high correlations within two groups of variables:\n",
    "- Year_Birth, Income, Frq, Mnt\n",
    "- NetPurchase, CatPurchase, Year_Birth\n",
    "\n",
    "Now it is time to understand the ones that we should keep and the ones to be removed. <br>\n",
    "\n",
    "The remaining methods that we are going to apply  for feature selection are: \n",
    "- __Wrapper methods__ - RFE\n",
    "- __Embedded Methods__ - Lasso Regression and Decision Trees\n",
    "\n",
    "It is important to note that regression models are particularly sensible to correlated features. Therefore, we will begin by applying decision trees to identify which features among the correlated ones should be excluded. \n",
    "\n",
    "Once we have eliminated the correlated features, we can proceed with techniques that involve regression models, such as RFE (using Logistic Regression) and Lasso Regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683887b",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"10th-bullet\">\n",
    "\n",
    "__`3.1.2.3. Decision Trees (Embedded Method) - For continuous, ordinal and binary data`__\n",
    "\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1533629e",
   "metadata": {},
   "source": [
    "Similarly to what we have done in the previous techniques, we will now examine the feature importance of the independent variables in the training data  using a decision tree, regardless of the specific train subset being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b50eb",
   "metadata": {},
   "source": [
    "__`Step 17`__ - Create a function named as `plot_importance` that receives the feature importances and the name of the model being applied and: <br>\n",
    "- Sort the feature importances using sort_values()\n",
    "- create a figure with figsize=(4,5)  <br>\n",
    "- plot an horizontal bar with the results <br>\n",
    "- Add the title (\"Feature importance using \" and the name of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb53615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(variables,name):\n",
    "    imp_features = variables.sort_values()\n",
    "    plt.figure(figsize=(4,5))\n",
    "    imp_features.plot(kind = \"barh\")\n",
    "    plt.title(\"Feature importance using \" + name + \" Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c03b28",
   "metadata": {},
   "source": [
    "__`Step 18`__ - Create a function named `apply_dt` that takes your training data as input (independent variables and target). In this function:\n",
    "- Create a DecisionTreeClassifier with random_state = 99 and fit to your training data, Name it as `dt`\n",
    "- Call the attribute from the model decision trees `feature_importances_` and store those results in a Pandas DataFrame.\n",
    "- Call the previously defined function `plot_importance` to generate the horizontal bar with the feature importances provided by the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dt(X_train, y_train):\n",
    "    dt = DecisionTreeClassifier(random_state = 99).fit(X_train, y_train)\n",
    "    feature_importances = pd.Series(dt.feature_importances_, index = X_train.columns)\n",
    "    plot_importance(feature_importances, 'DT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f66b6ce",
   "metadata": {},
   "source": [
    "__`Step 19`__ - Create a function named as `select_best_features_dt` that receives the independent variables (X) and the target (y). <br>\n",
    "Inside of this function, follow these steps:<br>\n",
    "- Apply the previously defined StratifiedKFold as \"skf\" for cross-validation.\n",
    "- Fill missing data, focusing this time on the numerical variables.\n",
    "- Drop the `Marital_Status`variable, since we are using the feature importance of decision trees to evaluate the numerical data.\n",
    "- Call the `apply_dt` function on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c5f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features_dt(X, y):\n",
    "    count = 1\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        ####################################### FILL MISSING DATA #################################################\n",
    "        # Fill missing values in training data\n",
    "        X_train['Income'].fillna(X_train['Income'].median(), inplace = True)\n",
    "        \n",
    "        ######################################### SELECT FEATURES #################################################        \n",
    "        print('_________________________________________________________________________________________________\\n')\n",
    "        print('                                     SPLIT ' + str(count) + '                                    ')\n",
    "        print('_________________________________________________________________________________________________')\n",
    "\n",
    "        # check which features to use using decision Tree\n",
    "        X_train = X_train.drop(['Marital_Status'], axis = 1)\n",
    "        apply_dt(X_train, y_train)\n",
    "        \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2c1d7e",
   "metadata": {},
   "source": [
    "__`Step 19B`__ - Call your function `select_best_features_dt` passing as arguments your independent variables and your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ec7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_best_features_dt(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c31aa8",
   "metadata": {},
   "source": [
    "We had identified previously with correlation matrix that two groups of variables that seem to be highly correlated among them, namely:\n",
    "- Year_Birth, Income, Frq, Mnt\n",
    "- NetPurchase, CatPurchase, Year_Birth\n",
    "\n",
    "From the first group, we are going to include only Mnt, the variable that according to decision trees feature importance appears three times as the most important one. We remove `Year_Birth`, `Income` and `Frq`.\n",
    "\n",
    "From the second group (and not considering Year_birth since this variable have a high correlation with Mnt), we are going to keep NetPurchase, that appears always more important than CatPurchase except in the last split. We remove `CatPurchase`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e4992",
   "metadata": {},
   "source": [
    "__`Step 20`__ - Remove the variables that were excluded by the combination of the results of the decision tree and the correlation matrix, namely `Income`, `Frq`, `CatPurchase` and `Year_Birth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec164108",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Income','Frq','CatPurchase','Year_Birth'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b58b5",
   "metadata": {},
   "source": [
    "At this moment, you already removed correlated features and can apply techniques where regression is used.\n",
    " <br><br>\n",
    " To apply Lasso Regression and RFE (using Logistic Regression as an estimator), we should scale our data previously. <br>\n",
    " In the case of the Decision Trees, it is not needed - the results should be similar if data is scaled or not. <br><br>\n",
    "\n",
    "<a class=\"anchor\" id=\"11th-bullet\">\n",
    "\n",
    "__`3.1.2.4. Recursive Feature Elimination (RFE) (Wrapper Method) - For continuous, ordinal and binary data`__\n",
    "    \n",
    "</a>\n",
    "\n",
    "__`Step 21`__ - Create a function named `apply_rfe` that takes your training data as input (independent variables and target). In this function:\n",
    "- Create an instance of RFE named as `rfe`, where `estimator = LogisticRegression()` and `n_features_to_select = 5` \n",
    "- Apply the method `fit_transform()` from `rfe` to your training data.\n",
    "- Call the attribute from the RFE named `support_` and store those results in a Pandas Series named `selected_features`.\n",
    "- Print `selected_features`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rfe(X_train, y_train):\n",
    "    rfe = RFE(estimator = LogisticRegression(), n_features_to_select = 5)\n",
    "    rfe.fit_transform(X = X_train, y = y_train)\n",
    "    selected_features = pd.Series(rfe.support_, index = X_train.columns)\n",
    "    print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91c397",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"12th-bullet\">\n",
    "\n",
    "__`3.1.2.5. Lasso - For continuous and ordinal data`__\n",
    "    \n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5af73cb",
   "metadata": {},
   "source": [
    "__`Step 22`__ - Create a function named `apply_lasso` that takes your training data as input (independent variables and target). In this function:\n",
    "- Remove the variable `Dependents` from the training data, since Lasso regression is not appropriate for binary data. \n",
    "- Create an instance of LogisticRegressionCV, and apply the method `fit()`to your training data.\n",
    "- Call the attribute from the Lasso named `coef_` and store those results in a Pandas Series named `coef`. This will return the coefficients associated with each variable.\n",
    "- Call the previously defined function `plot_importance` to generate the horizontal bar with the coefficients provided by the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de4c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lasso(X_train, y_train):\n",
    "    X_train = X_train.drop(['Dependents'], axis = 1)\n",
    "    lasso = LogisticRegressionCV(penalty='l1', solver='saga').fit(X_train, y_train)\n",
    "    coef = pd.Series(abs(lasso.coef_[0]), index = X_train.columns)\n",
    "    plot_importance(coef,'Lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e0c47",
   "metadata": {},
   "source": [
    "Now it is time to apply the different techniques to our data to each partition, and get insights from it. We can also apply again Decision Trees to obtain our final results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6f8dc",
   "metadata": {},
   "source": [
    "__`Step 23`__ - Create a function named as `select_best_features` that receives the independent variables (X) and the target (y). <br>\n",
    "Inside of this function, follow these steps:<br>\n",
    "- Apply the previously defined StratifiedKFold as \"skf\" for cross-validation.\n",
    "- Drop the `Marital_Status`variable, since we are evaluating the numerical data.\n",
    "- Apply MinMaxScaler to your training data.\n",
    "- Call the `apply_rfe` function on your data.\n",
    "- Call the `apply_lasso` function on your data. \n",
    "- Call the `apply_dt` function on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features(X,y):\n",
    "    count = 1\n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        ####################################### FILL MISSING DATA #################################################\n",
    "        # Income was already been removed from our variables\n",
    "        \n",
    "        ########################################### SCALE DATA #################################################### \n",
    "        numerical_data = X_train.drop(['Marital_Status'], axis = 1).copy()\n",
    "        scaler = MinMaxScaler().fit(numerical_data)\n",
    "        X_train_scaled = scaler.transform(numerical_data)\n",
    "        X_train_scaled = pd.DataFrame(X_train_scaled, columns = numerical_data.columns)\n",
    "    \n",
    "        ######################################### SELECT FEATURES #################################################        \n",
    "        print('_________________________________________________________________________________________________\\n')\n",
    "        print('                                     SPLIT ' + str(count) + '                                    ')\n",
    "        print('_________________________________________________________________________________________________')\n",
    "        \n",
    "        # Check which features to use using RFE\n",
    "        print('')\n",
    "        print('----------------- RFE ----------------------')\n",
    "        apply_rfe(X_train_scaled, y_train)\n",
    "        \n",
    "        # check which features to use using lasso\n",
    "        print('')\n",
    "        print('----------------- LASSO ----------------------')\n",
    "        apply_lasso(X_train_scaled, y_train)\n",
    "            \n",
    "        # check which features to use using lasso\n",
    "        print('')\n",
    "        print('----------------- DT ----------------------')\n",
    "        apply_dt(X_train_scaled, y_train)\n",
    "            \n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd16a81",
   "metadata": {},
   "source": [
    "__`Step 23B`__ - Call your function `select_best_features` passing as arguments your independent variables and your target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e344976",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_best_features(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844f3ea",
   "metadata": {},
   "source": [
    "At the end, you can combine the results you obtained previously to understand which features are the most important ones. <br>\n",
    "In Decision Trees and in Lasso, we are going to select the the Top 5.\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "### Numerical Data\n",
    "\n",
    "`Which variables should we keep using this techniques?`\n",
    "\n",
    "| Predictor | RFE | Lasso | DT | What to do? (One possible way to \"solve\") | \n",
    "| --- | --- | --- | --- | --- |\n",
    "Dependents      | 3 YES | X | 0 YES |REMOVE |\n",
    "Rcn             | 0 YES | 1 YES | 3 YES | Try with and without |\n",
    "Mnt             | 5 YES | 5 YES | 5 YES | KEEP |\n",
    "Clothes         | 5 YES | 5 YES | 4 YES | KEEP |\n",
    "Kitchen         | 2 YES | 0 YES | 1 YES | REMOVE |\n",
    "SmallAppliances | 0 YES | 1 YES | 4 YES | Try with and Without |\n",
    "HouseKeeping    | 1 YES | 0 YES | 2 YES | REMOVE |\n",
    "Toys            | 1 YES | 0 YES | 1 YES | REMOVE |\n",
    "NetPurchase     | 5 YES | 1 YES | 5 YES | KEEP |\n",
    "Recomendation   | 4 YES | 0 YES | 0 YES | Try with and Without |\n",
    "CostPerContact | | | | REMOVE - Constant Feature|\n",
    "RevenuePerPositiveAnswer | | | | REMOVE - Constant Feature|\n",
    "Income | | | | REMOVE (Highly correlated with Mnt) |\n",
    "Frq | |  | | REMOVE (Highly Correlated with Mnt) |\n",
    "CatPurchase | |  | | REMOVE (Highly Correlated with NetPurchase) |\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baac42",
   "metadata": {},
   "source": [
    "At this stage, we can identify certain variables that appear to be more important to retain, specifically `Mnt`, `Clothes`, `NetPurchase` and `Marital_Status`.\n",
    "\n",
    "However, there are other variables for which we are less certain about their significance: `Rcn`, `SmallAppliances`, `Toys`, and `Recomendation`.\n",
    "\n",
    "__`Step 24`__ Let's create two datasets:\n",
    "- One dataset that includes the variables that we are certain are important (name it as `keep_data`):\n",
    "    - Variables: Mnt, Clothes, NetPurchase, Marital_Status, DepVar <br><br>\n",
    "- Another dataset that includes the variables that are both important and those we are unsure about at this point (name it as `all_data`):\n",
    "    - Variables: Mnt, Clothes, NetPurchase, Rcn, SmallAppliances, Toys, Recomendation, Marital_Status, DepVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4dd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_data = data[['Mnt','Clothes','NetPurchase','Marital_Status','DepVar']].copy()\n",
    "all_data = data[['Mnt','Clothes','NetPurchase','Rcn','SmallAppliances','Recomendation','Marital_Status','DepVar']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf060eb9-0e4d-4ce4-9959-a2ed4469f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_data.to_csv('keep_data.csv')\n",
    "all_data.to_csv('all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800040f",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 4 & 5. Model & Assess (Modelling and Assessment)\n",
    "\n",
    "</a>\n",
    "<img src=\"Images/step4.png\" style=\"height:70px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5ef49",
   "metadata": {},
   "source": [
    "### 4.1. Model Selection\n",
    "\n",
    "In this section you should take the time to train different predictive algorithms with the data that got to this stage and **use the approppriate model assessment metrics to decide which model you think is the best to address your problem**.\n",
    "\n",
    "**You are expected to present on your report the model performances of the different algorithms that you tested and discuss what informed your choice for a specific algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89421085",
   "metadata": {},
   "source": [
    "### 4.2. Model Optimization\n",
    "\n",
    "After selecting the best algorithm (set of algorithms), you can try to optimize the performance of your model by fiddling with the algorithms' hyper-parameters and select the options that result on the best overall performance.\n",
    "\n",
    "Possible ways of doing this can be through:\n",
    "1. [GridSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "2. [RandomSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05381828",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"\">\n",
    "\n",
    "# 5. Deploy\n",
    "\n",
    "</a>\n",
    "<img src=\"Images/step5.png\" style=\"height:70px\">\n",
    "\n",
    "\n",
    "### 5.0 Training a final model\n",
    "\n",
    "You used the previous steps of modelling and assessment to determine what would be best strategies when it comes to preprocessing, scaling, feature selection, algorithm and hyper-parameters you could find. \n",
    "\n",
    "**By this stage, all of those choices were already made**. For that reason, a split between training and validation is no longer necessary. **A good practice** would be to take the initial data and train a final model with all of the labeled data that you have available.\n",
    "\n",
    "**Everything is figured by this stage**, so, on a first level all you need to do is replicate the exact preprocessing, scaling and feature selection decisions you made before.<br>\n",
    "When it comes to the final model, all you have to do is creeate a new instance of your best algorithm with the best parameters that you uncovered (no need to try all algorithms and hyper-parameters again)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2aa065",
   "metadata": {},
   "source": [
    "### 5.1. Import and Transform your test data\n",
    "\n",
    "Remember, the test data does not have the `outcome` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d07b44",
   "metadata": {},
   "source": [
    "### 5.2. Obtain Predictions on the test data from your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4effcd3a",
   "metadata": {},
   "source": [
    "### 5.3. Create a Dataframe containing the index of each row and its intended prediction and export it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0983e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nome_do_ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
